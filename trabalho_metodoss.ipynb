{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método da Máxima Verossimilhança\n",
    "\n",
    "\n",
    "Ideia do campo matemático, usada inicialmente por matemáticos importantes como Gauss e Laplace e posteriormente, no campo da estatística, defendido pelo estatístico inglês Ronald Fisher.\n",
    "\n",
    "A função densidade de um vetor aleatório $(x_1,...,x_n)$ cuja distribuição depende de um parâmetro $\\theta$ é a função densidade de probabilidade conjunta dada por:\n",
    "\n",
    "$$fx_1,...,x_n(x_1,...,x_n,\\theta)$$\n",
    "\n",
    "Se forem amostras independentes temos $fx_1,...,x_n(x_1,...,x_n,\\theta) = fx_1(x_1,\\theta)fx_2(x_2,\\theta)...fx_n(x_n,\\theta)$\n",
    "\n",
    "E através de que a mesma amostra provém da mesma distribuição de $X$, podemos escrever da seguinte forma:\n",
    "\n",
    "$$fx_1,...,x_n(x_1,...,x_n,\\theta) = \\prod_{i=1}^{n}{fx_i(x_i,\\theta)}$$\n",
    "\n",
    "Quando observamos $x_1,...,x_n$ a função $fx_1,...,x_n(x_1,...,x_n,\\theta)$ passa a ser uma função do parâmetro $\\theta$, chamada de ***função verossimilhança*** e denotada por:\n",
    "\n",
    "$$L(\\theta) = L(\\theta;x) = \\prod_{i=1}^{n}{fx(x_i;\\theta)}$$\n",
    "\n",
    "A letra $L$ está associada ao termo em inglês ***likelihood*** traduzida como ***verossimilhança***\n",
    "\n",
    "O estimador de máxima verossimilhança de $\\theta$ é tal que $\\hat{\\theta}$ pertence ao espaço paramétrico e maximiza a função de verossimilhança $L(\\theta;x)$.\n",
    "\n",
    "Ideia : Devemos encontrar $\\theta$ de tal forma que os dados observados $x_1,...,x_n$ tenham maior (máximo) probabilidade de serem obtidos.\n",
    "\n",
    "### EXEMPLO (sem a necessidade de um método numérico)\n",
    "\n",
    "Aplicando Máxima Verossimilhança para estimar o parâmetro $\\theta$ da distribuição exponencial\n",
    "\n",
    "$$fx(x;\\theta)= \\theta \\exp^{-\\theta{x}}, x>0, \\theta >0$$\n",
    "\n",
    "Aplicando o produtório\n",
    "\n",
    "$L(\\theta) = \\prod_{i=1}^{n}{fx(x_i;\\theta)} = \\prod_{i=1}^{n}\\theta \\exp^{-\\theta{x_i}} = \\theta^{n}\\exp^{-\\theta\\sum_{i=1}^{n}x_i}$\n",
    "\n",
    "**OBS**: Maximizar $L(\\theta)$ é equivalente a maximizar $log{L(\\theta)}$, pois a função logarítima é monótona crescente. Assim, obtemos a função de ***Log Verossimilhança***, o procedimento do $\\log$ é mais conveniente e irá alcançar o mesmo máximo.\n",
    "\n",
    "$$l(\\theta) = \\log{L(\\theta)}$$\n",
    "\n",
    "Assim\n",
    "\n",
    "$l(\\theta) = \\log[\\theta^{n}\\exp^{-\\theta\\sum_{i=1}^{n}x_i}] = n\\log\\theta - \\theta\\sum_{i=1}^{n}x_i$\n",
    "\n",
    "Agora a primeira derivada para encontrar o ponto crítico\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial\\theta} = \\frac{n}{\\theta} - \\sum_{i=1}^{n}x_i$$\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial\\theta} = 0 \\iff 0 = \\frac{n}{\\hat\\theta} - \\sum_{i=1}^{n}x_i \\iff \\sum_{i=1}^{n}x_i = \\frac{n}{\\hat\\theta} \\iff \\hat\\theta = \\frac{1}{\\bar{x}}$$\n",
    "\n",
    "Vendo se será ponto de Máximo ou Mínimo, se for menor que 0 será ponto de Máximo, o contrário será ponto de Mínimo.\n",
    "\n",
    "$$\\frac{\\partial^{2}l}{\\partial\\theta^{2}} = \\frac{-n}{\\hat\\theta^{2}} < 0$$ Logo, Máximo\n",
    "\n",
    "$$\\hat\\theta = \\frac{1}{\\bar{x}}$$ é o estimador de Máxima Verossimilhança de $\\theta$\n",
    "\n",
    "### EXEMPLO (necessidade de um método numérico)\n",
    "\n",
    "Aplicando Máxima Verossimilhança para estimar o parâmetro $\\theta$ da distribuição Lindley unitária (UL)\n",
    "\n",
    "\n",
    "$$fx(x;\\theta) = \\frac{\\theta^{2}}{1+\\theta}(1+x)^{-3}\\exp\\bigg(-\\frac{\\theta x}{1-x}\\bigg),0<x<1,\\theta>0$$\n",
    "\n",
    "Aplicando o EMV na densidade\n",
    "\n",
    "verossimilhança:\n",
    "\n",
    "$$L(\\theta) = \\prod_{i=1}^{n}fx(xi;\\theta)=\\prod_{i=1}^{n}\\frac{\\theta^{2}}{1+\\theta}(1+xi)^{-3}\\exp\\bigg(-\\frac{\\theta xi}{1-xi}\\bigg) =\\frac{\\theta^{2n}}{(1+\\theta)^{n}}\\prod_{i=1}^{n}(1+xi)^{-3}\\exp\\bigg(\\sum_{i=1}^{n}-\\frac{\\theta xi}{1-xi}\\bigg)$$\n",
    "\n",
    "log-verossimilhança:\n",
    "\n",
    "$$l(\\theta)=2n\\log\\theta - n\\log(1+\\theta) + \\sum_{i=1}^{n}\\log(1+xi)^{-3} - \\sum_{i=1}^{n}\\frac{\\theta xi}{1-xi}$$\n",
    "\n",
    "primeira derivada:\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial\\theta} = \\frac{2n}{\\theta} - \\frac{n}{1+\\theta} - \\sum_{i=1}^{n}\\frac{xi}{1-xi}$$\n",
    "\n",
    "segunda derivada:\n",
    "\n",
    "$$\\frac{\\partial^{2}l}{\\partial\\theta^{2}} = - \\frac{2n}{\\theta^{2}} + \\frac{n}{(1+\\theta)^{2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "arquivo = \"dadosanalfa2.xlsx\"\n",
    "\n",
    "df = pd.read_excel(arquivo, delimiter=';')\n",
    "\n",
    "top10_df = df.sort_values(by='Taxa de analfabetismo - 25 a 29 anos de idade 2010', ascending=False).head(10)\n",
    "top10_df.style.hide_index()\n",
    "\n",
    "def newton(f, df, x_antigo, tolerancia = 1e-4):\n",
    "  erro       = 1\n",
    "\n",
    "  while erro > tolerancia:\n",
    "    x_novo   = x_antigo - f(x_antigo)/df(x_antigo)\n",
    "    erro     = abs(x_antigo - x_novo)/abs(x_antigo)\n",
    "    x_antigo = x_novo\n",
    "    print(f'Valor estimado de raiz: {x_novo} - erro: {erro}')\n",
    "\n",
    "  print(f'A raiz estimada é: {x_novo}')\n",
    "\n",
    "lindley = lambda x: (theta**2/(1+theta))*((1+x)**-3)*np.exp((-theta*x)/(1-x))\n",
    "f = lambda theta: ((2 * n) / theta) - (n / (1 + theta)) - np.sum(x/ (1 - x))\n",
    "derivf = lambda theta: -(2 * n) / theta**2 + n / ((1 + theta)**2)\n",
    "\n",
    "n = df.shape[0]\n",
    "x = df.iloc[:, 1]\n",
    "\n",
    "theta = np.linspace(0, 200, 1000)\n",
    "plt.plot(theta, f(theta), \"-\")\n",
    "plt.grid()\n",
    "plt.ylim(-20, 10)\n",
    "plt.show()\n",
    "\n",
    "theta0 = 50\n",
    "newton(f, derivf, theta0, 1e-4)\n",
    "\n",
    "def bissec(f, a, b, eps = 1e-4):\n",
    "    err = 100\n",
    "    x_ant = a\n",
    "    iteracao = 1\n",
    "    while err>eps:\n",
    "        x = (a+b)/2.0\n",
    "        err = abs(x - x_ant)/abs(x)\n",
    "        if f(a)*f(x)<0:\n",
    "            b = x\n",
    "        else:\n",
    "            a = x\n",
    "        x_ant = x\n",
    "        print (f'Iteração: {iteracao} | Raiz estimada: {x} | Erro: {err}')\n",
    "        iteracao +=1\n",
    "    return (x)\n",
    "\n",
    "a = 50\n",
    "b = 70\n",
    "bissec(f,a,b)\n",
    "\n",
    "theta = 53.1298828125\n",
    "\n",
    "x2 = np.linspace(0, 0.2, 1000)\n",
    "\n",
    "plt.hist(x, density=True, edgecolor='black', label='Dados')\n",
    "\n",
    "plt.plot(x2, lindley(x2), '-', color='red', linewidth=2, label='Função Lindley')\n",
    "\n",
    "plt.xlabel('Valores de X')\n",
    "plt.ylabel('Frequência')\n",
    "plt.tight_layout()\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.ylim(0, 50)\n",
    "plt.xlim(0,0.2)\n",
    "plt.title('Histograma dos dados e Função Lindley')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
